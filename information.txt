Some conclusions:
1)NLP has lots of techniques: Vectorizer, TD-IDF, Latent Semantics Analytics
2)The first step is to Vectorize each document, if we have several docs, then it turns into a matrix
2.1) Hence we talk about to transform CORPUS (Collection of documents) into a Space Vector Model (matrix)
2.2) This matrix is oftenly, in ML, a matrix of rows (documents) and columns (terms)
2.3) The rows can be each document, sentence or even the word it self, which will turn into a word vs word matrix.
3)The base is a matrix which can turn into different measures: TD-IDF for example
4)TD-IDF seems to be the base for LSA

Principal topics for this project:
-NLP, LSA, PCA, TD-IDF, Space Vector Model, EDA (Exploratory Data Analysis)
-NLP techniques (there are more...) :Sentiment analysis, Topic modeling and Text Generation
-Topic Modeling: Latent Dirichlet Allocation (LDA),-Non-Negative Matrix Factorization (NMF),LSA (Latent Semantic Analysis)
-When using topic modeling algorithms , it asks por "components", each component is related to the number of sources this may vary
for example, in thesis, if they are 20 K thesis it doesn't mean there should be 20 K components

Start ML with 10th period
   -Obtener TÃ©rmino frecuencia y TF IDF con los nuevos campos
   -Obtener top 100, top 50
   -Modelo principal
      text_content
      heading
      subject
      type_of_thesis